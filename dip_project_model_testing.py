# -*- coding: utf-8 -*-
"""dip-project-model-testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I9NlHXmoIshwexHbbOcUMTANw1lrk75J
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# --- Core Libraries ---
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

# --- Kaggle-Specific Library ---
import kagglehub

# ==============================================================================
# 1. DOWNLOAD AND SETUP DATASET
# ==============================================================================
print("--- Step 1: Setting up the dataset ---")

# Use the already downloaded dataset from training
# Get the dataset path from cache
try:
    dataset_path = kagglehub.dataset_download("vipoooool/new-plant-diseases-dataset")
    print(f"Using cached dataset from: {dataset_path}")
except Exception as e:
    print(f"Error accessing dataset: {e}")
    print("Please make sure the training script has completed successfully.")
    exit()

# Construct the correct paths
base_dir = os.path.join(dataset_path, 'New Plant Diseases Dataset(Augmented)', 'New Plant Diseases Dataset(Augmented)')
train_dir = os.path.join(base_dir, 'train')
# We will use the 'valid' directory as our test set for this evaluation
test_dir = os.path.join(base_dir, 'valid')

# Define image size and batch size
IMG_SIZE = 128
BATCH_SIZE = 32

# Get class names from the folder names in the training directory
class_names = sorted(os.listdir(train_dir))
num_classes = len(class_names)
print(f"\nFound {num_classes} classes.")

# ==============================================================================
# 2. LOAD THE TRAINED MODEL
# ==============================================================================
print("\n--- Step 2: Loading the pre-trained custom CNN model ---")
# This is the path where your training script saved the model
model_path = 'baseline_cnn_model.keras'

# Check if the model file exists before trying to load it
if os.path.exists(model_path):
    model = tf.keras.models.load_model(model_path)
    print("Model loaded successfully.")
    model.summary()
else:
    print(f"ERROR: Model file not found at {model_path}")
    print("Please make sure you have run the training script first and the model was saved correctly.")
    # Exit the script if the model isn't found
    exit()

# ==============================================================================
# 3. PREPARE THE TEST DATASET
# ==============================================================================
print("\n--- Step 3: Preparing the test data generator ---")
# For evaluation, we only need to rescale the images. No augmentation is applied.
# It is CRITICAL to set shuffle=False to ensure that the order of predictions
# matches the order of the true labels.
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False  # Do not shuffle the data
)

# ==============================================================================
# 4. EVALUATE AND PREDICT
# ==============================================================================
print("\n--- Step 4: Evaluating the model on the test data ---")
loss, accuracy = model.evaluate(test_generator)
print(f"Test Accuracy: {accuracy * 100:.2f}%")
print(f"Test Loss: {loss:.4f}")

print("\nGenerating predictions...")
# Get the predicted probabilities for each class
y_pred_probs = model.predict(test_generator)
# Get the predicted class index by finding the index with the highest probability
y_pred = np.argmax(y_pred_probs, axis=1)
# Get the true class indices
y_true = test_generator.classes

# ==============================================================================
# 5. GENERATE VISUALIZATIONS
# ==============================================================================
print("\n--- Step 5: Generating performance visualizations ---")

# --- Visualization 1: Classification Report ---
# This provides a detailed breakdown of precision, recall, and F1-score for each class.
print("\n--- Classification Report ---")
print(classification_report(y_true, y_pred, target_names=class_names))

# --- Visualization 2: Confusion Matrix Heatmap ---
# This is the best way to visualize which classes the model is confusing.
print("\n--- Generating Confusion Matrix ---")
conf_matrix = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(22, 22))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix for Custom CNN', fontsize=26, pad=20)
plt.ylabel('Actual Class', fontsize=22)
plt.xlabel('Predicted Class', fontsize=22)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.tight_layout()
plt.show()


# --- Visualization 3: Visualize Misclassified Images ---
# Seeing where the model fails can provide valuable insights.
print("\n--- Visualizing Misclassified Images ---")
# Find the indices of all misclassified images
misclassified_indices = np.where(y_pred != y_true)[0]

# Randomly select a few misclassified images to display (e.g., 9)
if len(misclassified_indices) > 0:
    random_indices = np.random.choice(misclassified_indices, size=min(9, len(misclassified_indices)), replace=False)

    plt.figure(figsize=(15, 15))
    plt.suptitle("Sample Misclassified Images", fontsize=24, y=0.97)

    for i, idx in enumerate(random_indices):
        plt.subplot(3, 3, i + 1)

        # Get the corresponding image file path
        image_path = test_generator.filepaths[idx]
        img = tf.keras.preprocessing.image.load_img(image_path, target_size=(IMG_SIZE, IMG_SIZE))

        # Get the true and predicted labels
        true_label = class_names[y_true[idx]].replace("_", " ")
        predicted_label = class_names[y_pred[idx]].replace("_", " ")

        plt.imshow(img)
        plt.title(f"True: {true_label}\nPredicted: {predicted_label}", fontsize=12, color='darkred')
        plt.axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()
else:
    print("No misclassified images found! The model is perfect on this test set.")

